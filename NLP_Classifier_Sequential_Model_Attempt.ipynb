{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Classifier: Sequential Model Attempt",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuDhTXN3A/P0ddFlfwPlun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojustxn/nlp-projects/blob/main/NLP_Classifier_Sequential_Model_Attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64toK-xeLTuX",
        "outputId": "5babafa9-5965-4879-db79-a4ed00e1ba87"
      },
      "source": [
        "# !pip uninstall -y tensorflow tf-nightly keras\n",
        "\n",
        "# !pip install -q -U tf-nightly\n",
        "# !pip install -q -U tensorflow-text-nightly\n",
        "!pip install tensorflow-text==2.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text==2.6.0\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.6.0) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.6.0) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.39.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.17.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (5.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.37.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.34.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.4.5)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text==2.6.0) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0_Rn1QStr-",
        "outputId": "291851ac-d8b2-49a9-8711-6bc8fbcebc7b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "tf.executing_eagerly()\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "import pathlib\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnsWMaVIS47P",
        "outputId": "3e488662-9791-453a-bf0c-426f825cd7e2"
      },
      "source": [
        "%%script echo skipping\n",
        "abalone_train = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
        "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "          \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
        "\n",
        "abalone_train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVRvB72sS9s9",
        "outputId": "75620b3f-c693-4650-c22d-234663f3ec8d"
      },
      "source": [
        "%%script echo skipping\n",
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Age')\n",
        "abalone_labels.head()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks52fdfsTxkT",
        "outputId": "a7db6891-2982-48ef-9dd9-1a2b3be63561"
      },
      "source": [
        "%%script echo skipping\n",
        "abalone_model = tf.keras.Sequential([\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)                     \n",
        "])\n",
        "\n",
        "abalone_model.compile(loss = tf.losses.MeanSquaredError(),\n",
        "                      optimizer = tf.optimizers.Adam())\n",
        "abalone_model.fit(abalone_features, abalone_labels, epochs=100)\n",
        "\n",
        "abalone_model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPdcOB4uVg8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249dfe6c-66fe-4894-94be-b6887d44e3a9"
      },
      "source": [
        "%%script echo skipping\n",
        "# abalone_model(abalone_features)\n",
        "abalone_features.to_numpy()\n",
        "abalone_np = abalone_features.to_numpy()\n",
        "print(abalone_np[:1,].shape)\n",
        "abalone_labels.head()\n",
        "abalone_model.predict(abalone_np[:80,])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDQguxqfAvZq",
        "outputId": "5e1ee37f-102d-4acc-a7c9-b2debc5da67d"
      },
      "source": [
        "TRAINING_URL = 'https://storage.googleapis.com/timely-train/train.txt'\n",
        "LABEL_URL = 'https://storage.googleapis.com/timely-train/label.txt'\n",
        "\n",
        "utils.get_file('train.txt', origin=TRAINING_URL)\n",
        "text_dir = utils.get_file('label.txt', origin=LABEL_URL)\n",
        "\n",
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "list(parent_dir.iterdir())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/timely-train/train.txt\n",
            "327680/327259 [==============================] - 0s 0us/step\n",
            "335872/327259 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/timely-train/label.txt\n",
            "180224/178972 [==============================] - 0s 0us/step\n",
            "188416/178972 [===============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.keras/datasets/train.txt'),\n",
              " PosixPath('/root/.keras/datasets/label.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihPEqTNpFAQg"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmDLy6jB2rH"
      },
      "source": [
        "# Combine the dataset and the labels\n",
        "lines_dataset = tf.data.TextLineDataset(str(parent_dir/'train.txt'))\n",
        "lines_labeled_dataset = tf.data.TextLineDataset(str(parent_dir/'label.txt'))\n",
        "\n",
        "# Sanity check\n",
        "# for i in lines_dataset:\n",
        "#   print(i)\n",
        "# Output: tf.Tensor(b'EART - Monday, Wednesday, Thursday, Friday, Sunday 07:46 - 08:43', shape=(), dtype=string)\n",
        "\n",
        "# print(type(lines_dataset))\n",
        "# Output: <class 'tensorflow.python.data.ops.readers.TextLineDatasetV2'>\n",
        "\n",
        "# Create a list of tuples containing (train, label) into a dataset\n",
        "# Label the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# all_labeled_data_set = tf.data.Dataset.from_tensor_slices(tensor1)\n",
        "# labeled_data_set = tf.data.Dataset.from_tensor_slices(list(map(lambda x, y: (x, y), tf.constant(lines_dataset, dtype=string), lines_labeled_dataset)))\n",
        "\n",
        "# Good since\n",
        "# Output: (<tf.Tensor: shape=(), dtype=string, numpy=b'BIOE - Monday, Tuesday, Wednesday, Friday, Saturday, Sunday from 12:12 to 12:57'>, <tf.Tensor: shape=(), dtype=string, numpy=b'[[1, 1, 1, 0, 1, 1, 1], 0.51, 0.54]'>)\n",
        "# But this is of type map, instead we want type dataset.\n",
        "# labeled_data_set = list(map(lambda x, y: (x, y), lines_dataset, lines_labeled_dataset))\n",
        "# print(labeled_data_set)\n",
        "lines_list = list(map(lambda x: x, lines_dataset))\n",
        "labels_list = list(map(lambda x: x, lines_labeled_dataset))\n",
        "\n",
        "# print(type(labeled_data_set))\n",
        "# Output: <class 'tensorflow.python.data.ops.readers.TextLineDatasetV2'>\n",
        "\n",
        "# Zip the lines and labels together\n",
        "# Create a list of labeled data\n",
        "# labeled_data_list = [(i,j) for i, j in zip(lines_list, lables_list)]\n",
        "# print(lines_list)\n",
        "labeled_dataset = tf.data.Dataset.from_tensor_slices((lines_list, labels_list))\n",
        "\n",
        "# for count,i in enumerate(lines_data_set.as_numpy_iterator()):\n",
        "#   print(count)\n",
        "# print(lines_data_set.cardinality())\n",
        "# for element in lines_data_set:\n",
        "#   print(element)\n",
        "\n",
        "# Zip the two data sets\n",
        "# labeled_data_set = tf.data.Dataset.zip((lines_data_set, labels_data_set))\n",
        "# print(type(labeled_data_set))\n",
        "# for i in labeled_data_set:\n",
        "#   print(i)\n",
        "#   print(i[0])\n",
        "\n",
        "# Output: (<tf.Tensor: shape=(), dtype=string, numpy=b'BIOE - Monday, Tuesday, Wednesday, Friday, Saturday, Sunday from 12:12 to 12:57'>, <tf.Tensor: shape=(), dtype=string, numpy=b'[[1, 1, 1, 0, 1, 1, 1], 0.51, 0.54]'>)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aycyuIrlFtcf"
      },
      "source": [
        "Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PImVQ28Fgdc"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 1000"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybMGzD-KJ0L5"
      },
      "source": [
        "labeled_dataset = labeled_dataset.shuffle(\n",
        "  BUFFER_SIZE, reshuffle_each_iteration=False\n",
        ")\n",
        "\n",
        "# print(type(labeled_dataset))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4tLg5xFnJHN"
      },
      "source": [
        "note: printf and print have different behaviours on a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMaJnoFrIlly",
        "outputId": "2af8466a-9716-49a7-b687-41b2d26301b8"
      },
      "source": [
        "for text, label in labeled_dataset.take(1):\n",
        "  print(\"Sentence: \", text)\n",
        "  print(\"Label: \", label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  tf.Tensor(b'ESCI: Monday, Saturday, Sunday 00:08 - 00:41', shape=(), dtype=string)\n",
            "Label:  tf.Tensor(b'[[1, 0, 0, 0, 0, 1, 1], 0.01, 0.03]', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcydeDhDHTP6"
      },
      "source": [
        "# Prepare the dataset for training\n",
        "To preprocess our text dataset, I use the [tf.text API](https://www.tensorflow.org/text) to tokenize, build a vocabulary and create a mapping between tokens to integers (StaticVocabularyTable) for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8CLqupsQcNa"
      },
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHa893BhoIz8"
      },
      "source": [
        "This function will lowercase and tokenize the text data set with the UnicodeScriptTokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWkbZ9frHro1"
      },
      "source": [
        "# Input from map as (Text, Label)\n",
        "def tokenize(text, unused_label):\n",
        "  lower_case = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lower_case)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyTNMG7fo1TK",
        "outputId": "39c160ce-563b-47a9-e713-dca503e8439f"
      },
      "source": [
        "tokenized_dataset = labeled_dataset.map(tokenize)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsI_mjcHpEH_",
        "outputId": "157b0832-f577-4445-8f8f-b2fd3d589e08"
      },
      "source": [
        "# for text_batch in tokenized_dataset.take(5):\n",
        "#   print(text_batch)\n",
        "print(tokenized_dataset.cardinality())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5000, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK4ew8dSGPPp"
      },
      "source": [
        "Build a vocabulary and sort the tokens by frequency, keep the top VOCAB_SIZE tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjlS7TnGHOai"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# AUTOTUNE = tf.data.AUTOTUNE class for configure_dataset used for performance\n",
        "# TODO: Find out what config_ds does and how it affects the dataset\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "tokenized_dataset = configure_dataset(tokenized_dataset)\n",
        "# help(configure_dataset)\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrNfPe0cLYOZ"
      },
      "source": [
        "Create the vocabulary frequency dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3ALR1rJIhC",
        "outputId": "d815f83d-968b-4245-a1cf-fb69172fa23c"
      },
      "source": [
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "\n",
        "for tokens in tokenized_dataset.as_numpy_iterator():\n",
        "  for tok in tokens:\n",
        "    vocab_dict[tok] +=1 \n",
        "\n",
        "# Sort by frequency, an example item is a tuple of the form (token, frequency)\n",
        "# Ascending order\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get only the tokens\n",
        "vocab = [token for token, count in vocab]\n",
        "\n",
        "# Get the VOCAB_SIZE highest frequency tokens\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "\n",
        "# Get the length of the vocab, account for the case where len(vocab) < VOCAB_SIZE\n",
        "vocab_size = len(vocab) \n",
        "\n",
        "print(\"First five vocab entries:\", vocab[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First five vocab entries: [b',', b'monday', b'tuesday', b'saturday', b'sunday', b'thursday', b'friday', b'wednesday', b'to', b'from']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1N1b043OCul"
      },
      "source": [
        "Use the vocab to create a StaticVocabularyTable, map the tokens to integers in the range (2, vocab_size + 2). \n",
        "\n",
        "For the ```TextVectorization``` layer,\n",
        "\n",
        "0 is reserved and will denote padding\n",
        "1 is reserved and will denote OOV (Out of vocabulary) token\n",
        "\n",
        "In"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvUYTDRALTd7"
      },
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2) # range(inclusive, exclusive) TODO: Revisit\n",
        "\n",
        "# Data used to initialize a StaticVocabularyTable\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64\n",
        ")\n",
        "\n",
        "# Number of buckets to use for OOV keys\n",
        "num_oov_buckets = 1\n",
        "\n",
        "# String to Id table that assigns out-of-vocabulary keys to hash buckets.\n",
        "# Create a table to map keys-values tensors.\n",
        "# key: tokens to values: integers\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bXo8xYDQ4dS",
        "outputId": "55d2c036-edc3-479b-c4a6-21dbb0040ba6"
      },
      "source": [
        "# standardize\n",
        "# tokenize\n",
        "# vectorize\n",
        "def preprocess_text(text, label):\n",
        "  standardized = tf_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standardized)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label\n",
        "\n",
        "sample, label = preprocess_text(\"test hello\", 1)\n",
        "print(sample)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([3902 3902], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20HvTGpYQ59D",
        "outputId": "3e62ce82-af3f-4ce0-e679-8882cb440fb3"
      },
      "source": [
        "example_text, example_label = next(iter(labeled_dataset))\n",
        "\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized: \",)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  b'ESCI: Monday, Saturday, Sunday 00:08 - 00:41'\n",
            "Vectorized: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsymd4nda2ni"
      },
      "source": [
        "Encoding all label data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVqr1vHQaUkE"
      },
      "source": [
        "all_encoded_data = labeled_dataset.map(preprocess_text)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo3K7ChpaoWh"
      },
      "source": [
        "# Split the dataset into train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-NY5uBldhQ2"
      },
      "source": [
        "\"For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\" Tf docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq7hyZIBcmBk",
        "outputId": "4f4d8470-32af-42e1-e006-911f5db823d6"
      },
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "print(train_data)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ShuffleDataset shapes: ((None,), ()), types: (tf.int64, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKq6D9abeQ6-"
      },
      "source": [
        "TODO: Use the Keras ```TextVectorization``` layer to batch and pad the vectorized data.\n",
        "\n",
        "\n",
        "Batches need to be of the same size and shape (each line of text has a different number of words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgkvw5oXarPW",
        "outputId": "c2213e76-3a12-4682-e98e-f39a72ae585d"
      },
      "source": [
        "# padded_batch: Combines consecutive elements of this dataset into padded batches.\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)\n",
        "\n",
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "# print(\"First text example: \", sample_text)\n",
        "# print(\"First label example: \", sample_labels)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text batch shape:  (64, 23)\n",
            "Label batch shape:  (64,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch1LDqQPko8_"
      },
      "source": [
        "vocab_size += 2 # To account for padding denoted with a 0, and OOV denoted with a 1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8NzhJKtn4N6"
      },
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_d8OYrmn89l"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MBZ_69Foswe"
      },
      "source": [
        " ```int``` vectorized layer to build a 1D CN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTsLb1aKoCJp"
      },
      "source": [
        "NUM_LABELS = 1\n",
        "\n",
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfKVesfFrQoY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "b7e34d43-b94e-4752-d1f5-6e7d8bf291d1"
      },
      "source": [
        "model = create_model(vocab_size=vocab_size, num_labels=3)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Issue #1: The labels are breaking the code\n",
        "# Labels are of type string\n",
        "# Issue #2: Sequential models cannot produce multiple outputs\n",
        "history = model.fit(train_data, validation_data=validation_data, epochs=3)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-661a13871703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics=['accuracy'])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node sparse_categorical_crossentropy/Cast (defined at <ipython-input-29-661a13871703>:7) ]] [Op:__inference_train_function_24132]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZcyQTkOv77T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}